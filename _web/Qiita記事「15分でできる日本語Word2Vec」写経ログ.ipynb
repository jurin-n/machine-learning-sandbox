{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートは、Qiitaの記事\n",
    "\n",
    "* 15分でできる日本語Word2Vec(https://qiita.com/makaishi2/items/63b7986f6da93dc55edd)\n",
    "\n",
    "を試したログです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 青空文庫より「三四郎」をダウンロードし整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl -LO http://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\n",
    "!unzip 794_ruby_4237.zip\n",
    "!ls -l sanshiro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "一\n",
      "\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌を\n",
      "\n",
      "\n",
      "取りかかる。与次郎だけが三四郎のそばへ来た。\n",
      "「どうだ森の女は」\n",
      "「森の女という題が悪い」\n",
      "「じゃ、なんとすればよいんだ」\n",
      "　三四郎はなんとも答えなかった。ただ口の中で迷羊、迷羊と繰り返した。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "# ファイル読込み、内部表現化\n",
    "f = codecs.open('sanshiro.txt', \"r\", \"sjis\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "# ファイル整形\n",
    "import re\n",
    "# ヘッダ部分の除去\n",
    "text = re.split('\\-{5,}',text)[2]\n",
    "# フッタ部分の除去\n",
    "text = re.split('底本：',text)[0]\n",
    "# | の除去\n",
    "text = text.replace('|', '')\n",
    "# ルビの削除\n",
    "text = re.sub('《.+?》', '', text)\n",
    "# 入力注の削除\n",
    "text = re.sub('［＃.+?］', '',text)\n",
    "# 空行の削除\n",
    "text = re.sub('\\n\\n', '\\n', text) \n",
    "text = re.sub('\\r', '', text)\n",
    "\n",
    "# 整形結果確認\n",
    "\n",
    "# 頭の100文字の表示 \n",
    "print(text[:100])\n",
    "# 見やすくするため、空行 \n",
    "print()\n",
    "print()\n",
    "# 後ろの100文字の表示 \n",
    "print(text[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三四郎\n",
      "京都\n",
      "ちょっと\n",
      "用\n",
      "ある\n",
      "降りる\n",
      "ついで\n",
      "一\n",
      "する\n",
      "目\n",
      "さめる\n",
      "女\n",
      "隣\n",
      "じいさん\n",
      "話\n",
      "始める\n",
      "いる\n"
     ]
    }
   ],
   "source": [
    "# Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenneizerインスタンスの生成 \n",
    "t = Tokenizer()\n",
    "\n",
    "# テキストを引数として、形態素解析の結果、名詞・動詞原型のみを配列で抽出する関数を定義 \n",
    "def extract_words(text):\n",
    "    tokens = t.tokenize(text)\n",
    "    return [token.base_form for token in tokens \n",
    "        if token.part_of_speech.split(',')[0] in['名詞', '動詞']]\n",
    "\n",
    "#  関数テスト\n",
    "ret = extract_words('三四郎は京都でちょっと用があって降りたついでに。')\n",
    "for word in ret:\n",
    "    print(word)\n",
    "\n",
    "# 全体のテキストを句点('。')で区切った配列にする。 \n",
    "sentences = text.split('。')\n",
    "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
    "word_list = [extract_words(sentence) for sentence in sentences]\n",
    "\n",
    "# 結果の一部を確認 \n",
    "for word in word_list[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備したデータを用いてWord2Vecのモデル作成、学習実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vecライブラリのロード\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# size: 圧縮次元数\n",
    "# min_count: 出現頻度の低いものをカットする\n",
    "# window: 前後の単語を拾う際の窓の広さを決める\n",
    "# iter: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
    "# model.wv.most_similarの結果が1に近いものばかりで、model.dict['wv']のベクトル値が小さい値ばかりの \n",
    "# ときは、学習回数が少ないと考えられます。\n",
    "# その場合、iterの値を大きくして、再度学習を行います。\n",
    "\n",
    "# 事前準備したword_listを使ってWord2Vecの学習実施\n",
    "model = word2vec.Word2Vec(word_list, size=100,min_count=5,window=5,iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習ずみモデル保存\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習モデルを使い類似単語の調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデル読み込み\n",
    "saved_model = word2vec.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.41022325e-01 -8.96081448e-01  1.17433119e+00 -7.06966996e-01\n",
      " -2.46490741e+00 -1.31385729e-01  1.03345025e+00 -1.13619041e+00\n",
      "  1.68432534e+00  2.73926854e+00 -1.47613585e+00  1.85458744e+00\n",
      "  2.65864044e-01 -6.10771120e-01  2.01002359e-01  2.05743361e+00\n",
      "  1.47974777e+00  1.96556032e+00  2.99169707e+00 -1.17292255e-01\n",
      " -1.38066101e+00 -3.75678957e-01  8.21240962e-01  1.12217879e+00\n",
      " -1.22123289e+00 -2.29579255e-01 -3.77498537e-01  2.58674502e+00\n",
      " -6.39067769e-01 -1.90476686e-01 -3.45091641e-01  3.04870367e+00\n",
      " -5.47300100e-01  1.36662871e-01 -3.14100742e+00  2.94421554e-01\n",
      " -2.56990838e+00 -8.83876503e-01 -1.19387627e+00 -2.94832647e-01\n",
      " -2.34409511e-01 -9.49867547e-01 -2.87450290e+00 -8.78191829e-01\n",
      " -1.49023294e+00  8.55758905e-01 -5.13252735e-01  1.71482980e+00\n",
      "  1.06291187e+00 -9.08660761e-04  7.88932145e-01 -1.47986925e+00\n",
      "  9.09207940e-01  2.36083180e-01 -8.90600204e-01 -3.02920914e+00\n",
      " -1.72716510e+00  1.00467406e-01 -6.27237856e-01 -1.09999585e+00\n",
      " -1.47139680e+00  1.30692744e+00 -2.27620912e+00 -1.06454062e+00\n",
      "  1.29074299e+00  1.24899179e-01 -1.25350487e+00 -1.52767515e+00\n",
      "  9.87490788e-02 -2.77907759e-01 -7.15479076e-01 -5.14481306e-01\n",
      " -1.39919770e+00 -1.89883202e-01 -8.65974501e-02  1.25232565e+00\n",
      " -8.18214417e-01 -1.59149957e+00  2.15319729e+00 -7.24504232e-01\n",
      " -2.54999697e-01 -5.31469822e-01  2.07446143e-01  9.40711617e-01\n",
      "  2.22212479e-01 -4.94568981e-02 -1.57062495e+00 -6.61760509e-01\n",
      " -4.12942559e-01 -1.02360868e+00  4.93354470e-01 -1.48864841e+00\n",
      "  1.38590264e+00  1.60173595e-01  5.17727137e-01  1.90769657e-01\n",
      " -1.26277137e+00 -9.21196222e-01  1.42796397e-01 -6.89339101e-01]\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認1\n",
    "# 一つ一つの単語は100次元のベクトルになっています。 \n",
    "# 「世間」のベクトル値を確認します。\n",
    "print(saved_model.__dict__['wv']['世間'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画工 0.35532063245773315\n",
      "玩具 0.3366878926753998\n",
      "迷子 0.3066449761390686\n",
      "便利 0.30430662631988525\n",
      "とき 0.2998444437980652\n",
      "テーブル 0.29242438077926636\n",
      "責任 0.28952527046203613\n",
      "独身 0.28797459602355957\n",
      "入口 0.2812340259552002\n",
      "別れる 0.27822446823120117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junnakano/anaconda/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認2\n",
    "# 関数most_similarを使って「世間」の類似単語を調べます \n",
    "ret = saved_model.wv.most_similar(positive=['京都']) \n",
    "for item in ret:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-sandbox",
   "language": "python",
   "name": "ml-sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
